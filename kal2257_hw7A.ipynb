{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Encoder-Decoder for Sequence–to–Sequence NMT"
      ],
      "metadata": {
        "id": "1FJRFz2ZPdz4"
      },
      "id": "1FJRFz2ZPdz4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abae0e4",
      "metadata": {
        "id": "0abae0e4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from random import choices\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76eb1756",
      "metadata": {
        "id": "76eb1756"
      },
      "outputs": [],
      "source": [
        "# Initialize some variables\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 10\n",
        "teacher_forcing_ratio = 0.5\n",
        "hidden_size = 256\n",
        "cell_size = 256\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions used to time the optimizations/plot results\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "0XukDB_nSuPx"
      },
      "id": "0XukDB_nSuPx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Vocabulary/Tokenize Text\n"
      ],
      "metadata": {
        "id": "i-MTnYc5QafN"
      },
      "id": "i-MTnYc5QafN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2771ae34",
      "metadata": {
        "id": "2771ae34"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a sentence by ' ' and return a list of the tokens (int ids) for each word\n",
        "# Use word2index\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    sentence=sentence.split(' ')\n",
        "    return [lang.word2index[x] for x in sentence]\n",
        "\n",
        "# Call the above on a sentence\n",
        "# After calling, add the EOS_token (int id) to the gotten list\n",
        "# Return a tensor, but reshape it so it's dimensions (-1, 1)\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes=indexes + [EOS_token]\n",
        "    tensor= torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "    return  tensor.view(1, -1)\n",
        "\n",
        "# For a source, target pair, call the above. Return a tuple of 2 tensors, one input_tensor and another an output_tensor\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "-WFkh5f4UCf7"
      },
      "id": "-WFkh5f4UCf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "HYiDj5xMQY8g"
      },
      "id": "HYiDj5xMQY8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64a592d",
      "metadata": {
        "id": "a64a592d"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/MyDrive/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs[0:10])\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247eaa96",
      "metadata": {
        "id": "247eaa96"
      },
      "outputs": [],
      "source": [
        "# Only read in data with these prefixes so we have easier data to deal with\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Only consider data where pair[0] and pair[1] have length less than MAX_LENGTH\n",
        "# Split on space first here also as MAX_LENGTH means the number of tokens (words), not number of chars\n",
        "def filterPair(p):\n",
        "    p0 =p[0].split()\n",
        "    p1 =p[1].split()\n",
        "    if len([item for item in eng_prefixes if item in p[1]])>0 and len(p0) < MAX_LENGTH and  len(p1) < MAX_LENGTH :\n",
        "      return p\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a347c50b",
      "metadata": {
        "id": "a347c50b",
        "outputId": "c204eb01-8e10-47c0-c418-cb8ba7eee825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "[['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !'], ['help !', 'a l aide !'], ['jump .', 'saute .'], ['stop !', 'ca suffit !'], ['stop !', 'stop !'], ['stop !', 'arrete toi !']]\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11915 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4575\n",
            "eng 2926\n",
            "['nous n en sommes pas encore convaincus .', 'we re not convinced yet .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model: LSTM Encoder-Decoder\n"
      ],
      "metadata": {
        "id": "elLxFtb4RCuZ"
      },
      "id": "elLxFtb4RCuZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138cf5e5",
      "metadata": {
        "id": "138cf5e5"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize the embedding and lstm; use batch_first=True\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Get the embeddings and reshape to be (1, 1, -1)\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        output = embedded\n",
        "        # Push through the lstm\n",
        "        output, hidden_cell = self.lstm(embedded)\n",
        "        return output, hidden_cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d6ee6c",
      "metadata": {
        "id": "c1d6ee6c"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize the embedding\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # Initialize an LSTM with yt and kt dimensions hidden_size\n",
        "        # Use batch_first=True\n",
        "        self.lstm =  nn.LSTM( hidden_size, hidden_size, batch_first=True)\n",
        "        # Initialize a Linear layer going to the appropriate vocabulary size\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # Optional: If you use NLLLoss, initialize LogSoftmax here\n",
        "        # What dimension?\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden_cell):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        # Push through relu\n",
        "        output = F.relu(output)\n",
        "        # Push output and hidden_cell tuple through the lstm\n",
        "        output, hidden_cell = self.lstm(output, hidden_cell)\n",
        "        # Apply LogSoftmax to output (?)\n",
        "        # Note you canleave this out if you wor with logits and CrossEntropyLoss\n",
        "        output = self.out(output)\n",
        "        return output, hidden_cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        # Used to initialize the hidden state (or cell state) to a tensor of dimension (1, 1, hidden_size)\n",
        "        # Just return a tensor here\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n"
      ],
      "metadata": {
        "id": "rl3LSUFGSYCU"
      },
      "id": "rl3LSUFGSYCU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d6a39b",
      "metadata": {
        "id": "c1d6a39b"
      },
      "outputs": [],
      "source": [
        "# For 50% of pairs, use teacher forcing so that we predict (y_1, y_2, ..., y_{T}) from (y_0, y_1, ..., y_{T-1}) on the decoder side\n",
        "# Without teacher forcing, we start with y_0 = SOS_token and then use \\hat{y}_1, the prediction at time step 0 as the input to time step 1 on the decoder side\n",
        "# For this case, we'll predict (\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_{T}) from (y_0, \\hat{y}_1, ..., \\hat{y}_{T-1}) on the decoder side\n",
        "# The crucial thing to realize here is that \\hat{y}_t is stochastic, and dependent on what the model predicts - mistakes propegate!\n",
        "\n",
        "def train(\n",
        "    input_tensor,\n",
        "    target_tensor,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    encoder_optimizer,\n",
        "    decoder_optimizer,\n",
        "    criterion,\n",
        "    max_length=MAX_LENGTH\n",
        "):\n",
        "    # Initialize the hidden and cell states\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initHidden()\n",
        "\n",
        "    # Reset the optimizer gradients to 0\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Reverse the input here, see below\n",
        "    input_tensor = torch.flip(input_tensor,[1])\n",
        "\n",
        "    # If we want to predict [x, y, z] from [a, b, c], we should feed in [c, b, a] on the encoder side so that a is as close to x as possible\n",
        "    # The above trick was noted in making a big difference\n",
        "    for it in range(input_length):\n",
        "        encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "\n",
        "    # Initialize the decoder input to the SOS_token\n",
        "    decoder_input  = torch.empty(encoder_output.size(0), 1, dtype=torch.long, device=device)\n",
        "    decoder_input  = decoder_input.fill_(SOS_token)\n",
        "\n",
        "    # Initialize the hidden states of the decoder with the hidden states of the encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "\n",
        "    # For this pair, use teacher forcing with 50% probability, else don't\n",
        "    val=random.randint(0, 1)==1\n",
        "    use_teacher_forcing = val\n",
        "\n",
        "    target_length_used = 0\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        target_length_used = target_length\n",
        "\n",
        "        for jt in range(target_length):\n",
        "            # Push decoder_input, decoder_hidden, and decoder_cell through the decoder\n",
        "            decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "           # Update the loss\n",
        "            loss += criterion(decoder_output.view(-1, decoder_output.size(-1)),target_tensor[0][jt].view(1))\n",
        "            # Set the next decoder_input to the current y_t\n",
        "            decoder_input = target_tensor[:,jt]\n",
        "            decoder_input = decoder_input.unsqueeze(1)\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        # Note that in this case we feed in at most target_length words\n",
        "        # If, however, we predict EOS_token, we break out\n",
        "        # You need to also carefully get the target lenght used since it might not be target_length\n",
        "\n",
        "        for jt in range(target_length):\n",
        "            # As before\n",
        "            decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "\n",
        "            # Get the top index, \\hat{y}_t; this will be the next decoder_input\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze(-1)\n",
        "            decoder_input = decoder_input.detach()\n",
        "\n",
        "            loss += criterion(decoder_output.view(-1, decoder_output.size(-1)),target_tensor[0][jt].view(1))\n",
        "\n",
        "            # Update the target_length_used\n",
        "            target_length_used +=1\n",
        "\n",
        "            # If the EOS_token was generated, exit\n",
        "            if decoder_input == EOS_token:\n",
        "              break\n",
        "\n",
        "    # Collect gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Do a step; do this both for the encoder and the decoder\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return the loss for this pair. Note that you'll need to use target_length_used not target_length; why?\n",
        "    return loss.item()/target_length_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8763fc7e",
      "metadata": {
        "id": "8763fc7e"
      },
      "outputs": [],
      "source": [
        "def trainIters(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    n_iters,\n",
        "    print_every=1000,\n",
        "    plot_every=100,\n",
        "    learning_rate=0.01\n",
        "):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Initialize the encoder and decoder optimizers with the above learning rate\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get n_iters training pairs\n",
        "    # In this example, we are effectively doing SGD with batch size 1\n",
        "\n",
        "    training_pairs = choices(pairs,k=n_iters)\n",
        "\n",
        "    # The loss; either NLLLoss if you use log sigmoids or CrossEntropyLoss if you use logits\n",
        "    criterion =nn.CrossEntropyLoss()\n",
        "\n",
        "    for it in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[it - 1]\n",
        "\n",
        "        input_tensor = tensorsFromPair(training_pair)[0]\n",
        "        target_tensor = tensorsFromPair(training_pair)[1]\n",
        "\n",
        "        # Train on the input, target pair\n",
        "        loss = train(input_tensor,target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "        # Update the total loss and the plot loss\n",
        "        # We can plot and print at different granularities\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if it % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(\n",
        "                '%s (%d %d%%) %.4f' % (\n",
        "                    timeSince(start, it / n_iters),\n",
        "                    it, it / n_iters * 100, print_loss_avg)\n",
        "            )\n",
        "\n",
        "        if it % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "            showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Validation:"
      ],
      "metadata": {
        "id": "JwFV6uWVVDDi"
      },
      "id": "JwFV6uWVVDDi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1990413",
      "metadata": {
        "id": "d1990413"
      },
      "outputs": [],
      "source": [
        "# For a certain input, get the predicted output sentence\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # Transform the input sentence into a tensor\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        # Initilize the hidden and cell states of the LSTM\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initHidden()\n",
        "\n",
        "        # Run the data through the LSTM word by word manually\n",
        "        # At each step, feed in the input, the hidden state, and the cell state and calture the new hidden / cell states\n",
        "        for it in range(input_length):\n",
        "            encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "\n",
        "        # Initialize the decoder input with the SOS token\n",
        "        # This is y_0\n",
        "        decoder_input =SOS_token\n",
        "\n",
        "        # Initialize the decoder hidden and cell states with the final encoder hidden and cell states\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        decoded_words = []\n",
        "\n",
        "        for jt in range(max_length):\n",
        "            # As with the encoder run the \\hat{y}_{t-1}, hidden, and state cells through the decoder\n",
        "            # Capture the new hidden and cell states and the logits or log_softmax for the vocabulary\n",
        "            decoder_output, (decoder_hidden, decoder_cell) =  decoder(encoder_output, encoder_hidden)\n",
        "            # Get the top y for the decoder, this will be the new \\hat{y}_t which we can use at the next step\n",
        "            _, topi = decoder_output.topk(1)\n",
        "\n",
        "            # Put logic so that if we get topi == EOS_token, we add this and break\n",
        "            # Otherwise, we map the index topi to the word in output_lang via index2word\n",
        "            for i in topi.squeeze():\n",
        "              if EOS_token == i.item():\n",
        "                decoded_words=decoded_words + [EOS_token]\n",
        "                break\n",
        "              values=i.item()\n",
        "              decoded_words =decoded_words + [output_lang.index2word[values]]\n",
        "I\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Execution:"
      ],
      "metadata": {
        "id": "newHtxqpVLP5"
      },
      "id": "newHtxqpVLP5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9467ede",
      "metadata": {
        "id": "d9467ede",
        "outputId": "c1b48a43-23b2-4559-fbfe-f416b6685dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 4s (- 14m 58s) (5000 6%) 2.8336\n",
            "2m 6s (- 13m 41s) (10000 13%) 2.3788\n",
            "3m 16s (- 13m 5s) (15000 20%) 2.1190\n",
            "4m 22s (- 12m 1s) (20000 26%) 1.9225\n",
            "5m 25s (- 10m 51s) (25000 33%) 1.7266\n",
            "6m 30s (- 9m 46s) (30000 40%) 1.5689\n",
            "7m 36s (- 8m 41s) (35000 46%) 1.4171\n",
            "8m 39s (- 7m 34s) (40000 53%) 1.3130\n",
            "9m 42s (- 6m 28s) (45000 60%) 1.1897\n",
            "10m 45s (- 5m 22s) (50000 66%) 1.0888\n",
            "11m 49s (- 4m 17s) (55000 73%) 0.9790\n",
            "12m 52s (- 3m 13s) (60000 80%) 0.9047\n",
            "13m 55s (- 2m 8s) (65000 86%) 0.8080\n",
            "47m 11s (- 3m 22s) (70000 93%) 0.7385\n",
            "235m 55s (- 0m 0s) (75000 100%) 0.6681\n"
          ]
        }
      ],
      "source": [
        "# Initialize the encoder and decoder and run them through the trainIters function\n",
        "encoder =  EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder =  DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2e1060",
      "metadata": {
        "id": "3c2e1060"
      },
      "outputs": [],
      "source": [
        "# Here we will do an evaluation. Gather up n=7500 random pairs and for each pair get te BLEU score.\n",
        "# Don't use BLUE based on 4-grams, use just 2-grams with \"sentence_bleu\" in nltk - you can also find other implementations.\n",
        "# You should be a BLEU of about 10 or slightly more.\n",
        "def evaluateRandomly(encoder, decoder, n=7500, debug=False):\n",
        "    bleu_scores = []\n",
        "    for i in range(n):\n",
        "        # Randomly choose a pair of sentences\n",
        "        pair = random.choice(pairs)\n",
        "        if debug:\n",
        "            print('French Original: ', pair[0])\n",
        "            print('English Reference: ', pair[1])\n",
        "        # Leave out the <EOS> symbol\n",
        "        # Run the source French sentence through the encoder-decoder and get the output_words\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "\n",
        "        # If <EOS> is at the end of output_words, remove it\n",
        "        if output_words[-1]==EOS_token:\n",
        "          output_words=output_words[:-1]\n",
        "\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        # Get the BLEU score based on 1 and 2 grams (words, bigrams); use 50% weight on each\n",
        "        # Use pair[1] as the reference\n",
        "        score = sentence_bleu(output_sentence)\n",
        "        # Append the BLEU score you got to the list of BLEU scores you keep\n",
        "        bleu_scores=bleu_scores +[score]\n",
        "        if debug:\n",
        "            print('Candidate Translation: ', output_sentence)\n",
        "            print('BLEU: ', score)\n",
        "            print('')\n",
        "    # Return the mean of the BLEU scores\n",
        "    print('The mean BLEU score is: ', bleu_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4f81ab",
      "metadata": {
        "id": "6e4f81ab"
      },
      "outputs": [],
      "source": [
        "# BLEU > 10 % is expected here\n",
        "#evaluateRandomly(encoder, decoder)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}