{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0abae0e4",
      "metadata": {
        "id": "0abae0e4"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from random import choices\n",
        "# Change to MPS logic if on mac\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the data in your Google Drive\n",
        "# You ca get the data here: https://www.kaggle.com/competitions/titanic/data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MjLsWwFfB5Yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b2b8f0-0c37-4a04-8836-a1d5c0070bbd"
      },
      "id": "MjLsWwFfB5Yx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15cd0d19",
      "metadata": {
        "id": "15cd0d19"
      },
      "source": [
        "For this notebook, use the hints to fill in the missing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ca4546b6",
      "metadata": {
        "id": "ca4546b6"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL IN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "76eb1756",
      "metadata": {
        "id": "76eb1756"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "2771ae34",
      "metadata": {
        "id": "2771ae34"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "a64a592d",
      "metadata": {
        "id": "a64a592d"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/MyDrive/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs[0:10])\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "247eaa96",
      "metadata": {
        "id": "247eaa96"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "# We'll just read in data with these prefixes so we have easier data to deal with\n",
        "# These are \"target\" prefixes\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Only use pairs where the english data (pair[1]) has the prefix above\n",
        "# Also, only consider data where pair[0] and pair[1] have length less than MAX_LENGTH\n",
        "# Split on space first here also as MAX_LENGTH means the number of tokens (words), not number of chars\n",
        "def filterPair(p):\n",
        "    p0 =p[0].split()\n",
        "    p1 =p[1].split()\n",
        "    if len([item for item in eng_prefixes if item in p[1]])>0 and len(p0) < MAX_LENGTH and  len(p1) < MAX_LENGTH :\n",
        "      return p\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a347c50b",
      "metadata": {
        "id": "a347c50b",
        "outputId": "c204eb01-8e10-47c0-c418-cb8ba7eee825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "[['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !'], ['help !', 'a l aide !'], ['jump .', 'saute .'], ['stop !', 'ca suffit !'], ['stop !', 'stop !'], ['stop !', 'arrete toi !']]\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11915 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4575\n",
            "eng 2926\n",
            "['nous n en sommes pas encore convaincus .', 'we re not convinced yet .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "138cf5e5",
      "metadata": {
        "id": "138cf5e5"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize the embedding and lstm; use batch_first=True\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Get the embeddings and reshape to be (1, 1, -1)\n",
        "        # Why? remember we use batch size = 1 in this HW for simplicity\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        output = embedded\n",
        "        # Push through the lstm\n",
        "        output, hidden_cell = self.lstm(embedded)\n",
        "        return output, hidden_cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "c1d6ee6c",
      "metadata": {
        "id": "c1d6ee6c"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize the embedding\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # Initialize an LSTM with yt and kt dimensions hidden_size\n",
        "        # Use batch_first=True\n",
        "        self.lstm =  nn.LSTM( hidden_size, hidden_size, batch_first=True)\n",
        "        # Initialize a Linear layer going to the appropriate vocabulary size\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # Optional: If you use NLLLoss, initialize LogSoftmax here\n",
        "        # What dimension?\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden_cell):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        # Push through relu\n",
        "        output = F.relu(output)\n",
        "        # Push output and hidden_cell tuple through the lstm\n",
        "        output, hidden_cell = self.lstm(output, hidden_cell)\n",
        "        # Apply LogSoftmax to output (?)\n",
        "        # Note you canleave this out if you wor with logits and CrossEntropyLoss\n",
        "        output = self.out(output)\n",
        "        return output, hidden_cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        # Used to initialize the hidden state (or cell state) to a tensor of dimension (1, 1, hidden_size)\n",
        "        # Just return a tensor here\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "bfaf0877",
      "metadata": {
        "id": "bfaf0877"
      },
      "outputs": [],
      "source": [
        "# Split a sentence by ' ' and return a list of the tokens (int ids) for each word\n",
        "# Use word2index\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    sentence=sentence.split(' ')\n",
        "    return [lang.word2index[x] for x in sentence]\n",
        "\n",
        "# Call the above on a sentence\n",
        "# After calling, add the EOS_token (int id) to the gotten list\n",
        "# Return a tensor, but reshape it so it's dimensions (-1, 1)\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes=indexes + [EOS_token]\n",
        "    tensor= torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "    return  tensor.view(1, -1)\n",
        "\n",
        "# For a source, target pair, call the above. Return a tuple of 2 tensors, one input_tensor and another an output_tensor\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "c1d6a39b",
      "metadata": {
        "id": "c1d6a39b"
      },
      "outputs": [],
      "source": [
        "# For 50% of pairs, use teacher forcing so that we predict (y_1, y_2, ..., y_{T}) from (y_0, y_1, ..., y_{T-1}) on the decoder side\n",
        "# Without teacher forcing, we start with y_0 = SOS_token and then use \\hat{y}_1, the prediction at time step 0 as the input to time step 1 on the decoder side\n",
        "# For this case, we'll predict (\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_{T}) from (y_0, \\hat{y}_1, ..., \\hat{y}_{T-1}) on the decoder side\n",
        "# The crucial thing to realize here is that \\hat{y}_t is stochastic, and dependent on what the model predicts - mistakes propegate!\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(\n",
        "    input_tensor,\n",
        "    target_tensor,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    encoder_optimizer,\n",
        "    decoder_optimizer,\n",
        "    criterion,\n",
        "    max_length=MAX_LENGTH\n",
        "):\n",
        "    # Initialize the hidden and cell states\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initHidden()\n",
        "\n",
        "    # Reset the optimizer gradients to 0\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Reverse the input here, see below\n",
        "    input_tensor = torch.flip(input_tensor,[1])\n",
        "\n",
        "    # If we want to predict [x, y, z] from [a, b, c], we should feed in [c, b, a] on the encoder side so that a is as close to x as possible\n",
        "    # The above trick was noted in making a big difference\n",
        "    for it in range(input_length):\n",
        "        encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "\n",
        "    # Initialize the decoder input to the SOS_token\n",
        "    decoder_input  = torch.empty(encoder_output.size(0), 1, dtype=torch.long, device=device)\n",
        "    decoder_input  = decoder_input.fill_(SOS_token)\n",
        "\n",
        "    # Initialize the hidden states of the decoder with the hidden states of the encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "\n",
        "    # For this pair, use teacher forcing with 50% probability, else don't\n",
        "    val=random.randint(0, 1)==1\n",
        "    use_teacher_forcing = val\n",
        "\n",
        "    target_length_used = 0\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        target_length_used = target_length\n",
        "\n",
        "        for jt in range(target_length):\n",
        "            # Push decoder_input, decoder_hidden, and decoder_cell through the decoder\n",
        "            decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "           # Update the loss\n",
        "            loss += criterion(decoder_output.view(-1, decoder_output.size(-1)),target_tensor[0][jt].view(1))\n",
        "            # Set the next decoder_input to the current y_t\n",
        "            decoder_input = target_tensor[:,jt]\n",
        "            decoder_input = decoder_input.unsqueeze(1)\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        # Note that in this case we feed in at most target_length words\n",
        "        # If, however, we predict EOS_token, we break out\n",
        "        # You need to also carefully get the target lenght used since it might not be target_length\n",
        "\n",
        "        for jt in range(target_length):\n",
        "            # As before\n",
        "            decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
        "\n",
        "            # Get the top index, \\hat{y}_t; this will be the next decoder_input\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze(-1)\n",
        "            decoder_input = decoder_input.detach()\n",
        "\n",
        "            loss += criterion(decoder_output.view(-1, decoder_output.size(-1)),target_tensor[0][jt].view(1))\n",
        "\n",
        "            # Update the target_length_used\n",
        "            target_length_used +=1\n",
        "\n",
        "            # If the EOS_token was generated, exit\n",
        "            if decoder_input == EOS_token:\n",
        "              break\n",
        "\n",
        "    # Collect gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Do a step; do this both for the encoder and the decoder\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return the loss for this pair. Note that you'll need to use target_length_used not target_length; why?\n",
        "    return loss.item()/target_length_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "5b0a1103",
      "metadata": {
        "id": "5b0a1103"
      },
      "outputs": [],
      "source": [
        "# Helper functions used to time the optimizations\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "8763fc7e",
      "metadata": {
        "id": "8763fc7e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "def trainIters(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    n_iters,\n",
        "    print_every=1000,\n",
        "    plot_every=100,\n",
        "    learning_rate=0.01\n",
        "):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Initialize the encoder and decoder optimizers with the above learning rate\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get n_iters training pairs\n",
        "    # In this example, we are effectively doing SGD with batch size 1\n",
        "\n",
        "    training_pairs = choices(pairs,k=n_iters)\n",
        "\n",
        "    # The loss; either NLLLoss if you use log sigmoids or CrossEntropyLoss if you use logits\n",
        "    criterion =nn.CrossEntropyLoss()\n",
        "\n",
        "    for it in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[it - 1]\n",
        "\n",
        "        input_tensor = tensorsFromPair(training_pair)[0]\n",
        "        target_tensor = tensorsFromPair(training_pair)[1]\n",
        "\n",
        "        # Train on the input, target pair\n",
        "        loss = train(input_tensor,target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "        # Update the total loss and the plot loss\n",
        "        # We can plot and print at different granularities\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if it % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(\n",
        "                '%s (%d %d%%) %.4f' % (\n",
        "                    timeSince(start, it / n_iters),\n",
        "                    it, it / n_iters * 100, print_loss_avg)\n",
        "            )\n",
        "\n",
        "        if it % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "            showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "d1990413",
      "metadata": {
        "id": "d1990413"
      },
      "outputs": [],
      "source": [
        "# For a certain input, get the predicted output sentence\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # Transform the input sentence into a tensor\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        # Initilize the hidden and cell states of the LSTM\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initHidden()\n",
        "\n",
        "        # Run the data through the LSTM word by word manually\n",
        "        # At each step, feed in the input, the hidden state, and the cell state and calture the new hidden / cell states\n",
        "        for it in range(input_length):\n",
        "            encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "\n",
        "        # Initialize the decoder input with the SOS token\n",
        "        # This is y_0\n",
        "        decoder_input =SOS_token\n",
        "\n",
        "        # Initialize the decoder hidden and cell states with the final encoder hidden and cell states\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        decoded_words = []\n",
        "\n",
        "        for jt in range(max_length):\n",
        "            # As with the encoder run the \\hat{y}_{t-1}, hidden, and state cells through the decoder\n",
        "            # Capture the new hidden and cell states and the logits or log_softmax for the vocabulary\n",
        "            decoder_output, (decoder_hidden, decoder_cell) =  decoder(encoder_output, encoder_hidden)\n",
        "            # Get the top y for the decoder, this will be the new \\hat{y}_t which we can use at the next step\n",
        "            _, topi = decoder_output.topk(1)\n",
        "\n",
        "            # Put logic so that if we get topi == EOS_token, we add this and break\n",
        "            # Otherwise, we map the index topi to the word in output_lang via index2word\n",
        "            for i in topi.squeeze():\n",
        "              if EOS_token == i.item():\n",
        "                decoded_words=decoded_words + [EOS_token]\n",
        "                break\n",
        "              values=i.item()\n",
        "              decoded_words =decoded_words + [output_lang.index2word[values]]\n",
        "I\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "cell_size = 256\n",
        "# Initialize the encoder and decoder and run them through the trainIters function\n",
        "encoder =  EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder =  DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, 75000, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTfmiPSUHU9m",
        "outputId": "039adb39-0083-4230-c567-da74c6837654"
      },
      "id": "xTfmiPSUHU9m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 21s (- 18m 58s) (5000 6%) 0.8431\n",
            "2m 43s (- 17m 40s) (10000 13%) 0.3946\n",
            "4m 4s (- 16m 17s) (15000 20%) 0.3351\n",
            "5m 28s (- 15m 2s) (20000 26%) 0.2628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5aed258",
      "metadata": {
        "id": "c5aed258"
      },
      "source": [
        "Fill in the evaluation function using the hints below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9467ede",
      "metadata": {
        "id": "d9467ede",
        "outputId": "c1b48a43-23b2-4559-fbfe-f416b6685dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 4s (- 14m 58s) (5000 6%) 2.8336\n",
            "2m 6s (- 13m 41s) (10000 13%) 2.3788\n",
            "3m 16s (- 13m 5s) (15000 20%) 2.1190\n",
            "4m 22s (- 12m 1s) (20000 26%) 1.9225\n",
            "5m 25s (- 10m 51s) (25000 33%) 1.7266\n",
            "6m 30s (- 9m 46s) (30000 40%) 1.5689\n",
            "7m 36s (- 8m 41s) (35000 46%) 1.4171\n",
            "8m 39s (- 7m 34s) (40000 53%) 1.3130\n",
            "9m 42s (- 6m 28s) (45000 60%) 1.1897\n",
            "10m 45s (- 5m 22s) (50000 66%) 1.0888\n",
            "11m 49s (- 4m 17s) (55000 73%) 0.9790\n",
            "12m 52s (- 3m 13s) (60000 80%) 0.9047\n",
            "13m 55s (- 2m 8s) (65000 86%) 0.8080\n",
            "47m 11s (- 3m 22s) (70000 93%) 0.7385\n",
            "235m 55s (- 0m 0s) (75000 100%) 0.6681\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "cell_size = 256\n",
        "# Initialize the encoder and decoder and run them through the trainIters function\n",
        "encoder =  EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder =  DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder, decoder, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0596845",
      "metadata": {
        "id": "c0596845"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3456cd29",
      "metadata": {
        "id": "3456cd29"
      },
      "source": [
        "Here we will do an evaluation.\n",
        "Gather up n=7500 random pairs and for each pair get te BLEU score.\n",
        "For this exercise, don't use BLUE based on 4-grams, use just 2-grams (you need to figure out how to specify this in $sentence\\_bleu$ below).\n",
        "Investigate how this can be done with \"sentence_bleu\" in nltk - you can also find other implementations.\n",
        "Print the average BLEU score after you've randomly drawn the sentences. You should be a BLEU of about 10 or slightly more.\n",
        "Note that is is training BLEU, which is all I'd like you to get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "3c2e1060",
      "metadata": {
        "id": "3c2e1060"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=7500, debug=False):\n",
        "    bleu_scores = []\n",
        "    for i in range(n):\n",
        "        # Randomly choose a pair of sentences\n",
        "        pair = random.choice(pairs)\n",
        "        if debug:\n",
        "            print('French Original: ', pair[0])\n",
        "            print('English Reference: ', pair[1])\n",
        "        # Leave out the <EOS> symbol\n",
        "        # Run the source French sentence through the encoder-decoder and get the output_words\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "\n",
        "        # If <EOS> is at the end of output_words, remove it\n",
        "        if output_words[-1]==EOS_token:\n",
        "          output_words=output_words[:-1]\n",
        "\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        # Get the BLEU score based on 1 and 2 grams (words, bigrams); use 50% weight on each\n",
        "        # Use pair[1] as the reference\n",
        "        score = sentence_bleu(output_sentence)\n",
        "        # Append the BLEU score you got to the list of BLEU scores you keep\n",
        "        bleu_scores=bleu_scores +[score]\n",
        "        if debug:\n",
        "            print('Candidate Translation: ', output_sentence)\n",
        "            print('BLEU: ', score)\n",
        "            print('')\n",
        "    # Return the mean of the BLEU scores\n",
        "    print('The mean BLEU score is: ', bleu_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "3277fc96",
      "metadata": {
        "id": "3277fc96",
        "outputId": "d440fc57-d65c-47e8-9020-db055137c935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-be088aa33bd0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BLEU > 10 % is expected here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-02f121b36807>\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n, debug)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Leave out the <EOS> symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Run the source French sentence through the encoder-decoder and get the output_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# If <EOS> is at the end of output_words, remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-f852b27f0e32>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# As with the encoder run the \\hat{y}_{t-1}, hidden, and state cells through the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Capture the new hidden and cell states and the logits or log_softmax for the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Get the top y for the decoder, this will be the new \\hat{y}_t which we can use at the next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-2a83f7666a70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden_cell)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Push through relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\" cuckoo Kaka\",hidden_cell)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ],
      "source": [
        "# BLEU > 10 % is expected here\n",
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4f81ab",
      "metadata": {
        "id": "6e4f81ab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}